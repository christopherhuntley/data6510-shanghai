{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Dolan.png\" width=\"180px\" align=\"right\">\n",
    "\n",
    "# **DATA 6510**\n",
    "# **Lesson 6: Big Data Pipelines** \n",
    "_A Visit to the Data Sausage Factory_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rHzwUorF5ab"
   },
   "source": [
    "## **Learning Objectives**\n",
    "### **Theory / Be able to explain ...**\n",
    "- The seven Big Data challenges\n",
    "- The function of each stage of the Source $\\rightarrow$ Lake $\\rightarrow$ Warehouse $\\rightarrow$ Mart $\\rightarrow$ Apps data pipeline\n",
    "- The basic elements of an ETL process\n",
    "- The purpose and advantages of columnar database technology\n",
    "\n",
    "### **Skills / Know how to ...**\n",
    "- Create Pivot Table-like queries in SQL from any Dimensional Data Warehouse\n",
    "- Use SQL to import online data into Tableau\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **BIG PICTURE: Business Intelligence is the tip of a long spear**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business intelligence (BI) is about generating actionable insights from data. It informs decision making at all levels of a firm: Given what we know now, how did it get that way and what can we do to get the results we desire? In other words, how can we use data to make us *smarter*? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to draw a distinction between BI and the broader world of Business Analytics. BI is **descriptive** (about things that are happening or have already happened) and to some degree **prescriptive** (what can be done now) but **very rarely predictive**. Where it does employ predictions, the models are more likely to be developed and tuned by data scientists with highly specialized training in machine learning and computer science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *apropos* analogy is the difference between accounting and finance. Accounting models are about accurately capturing the *past* and present (so it can be distilled into financial reports), while financial models are about status and *future* (so we can make financial decisions that affect the future). While it is certainly possible for an accountant to know a lot about finance and a financial analyst to know a lot of accounting, they are nonetheless different professions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard not to chuckle when people lump business intelligence in with database systems. Data visualization, probably *the* key feature for most BI apps, is in the presentation tier, not the data tier. Given access to lots of preprocessed data, with all the anomalies smoothed out, a BI dashboard allows us to see whatever stories the data can tell. That's very important $-$ literally why we collect, clean, and protect the data in the first place $-$ but about as much like a database app as whiffle ball is to baseball. They really are different things, with vastly different skill sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BI Pipelines](./img/L9_Data_Pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of collecting, curating, and repackaging data for consumption by business analysts is called a **pipeline**. Starting with raw data from various sources $-$ more on that in a moment $-$ the process flows through several layers of processing to distill the data down to just what the business analyst needs. While much of this \"sausage making\" process is designed and managed by data engineers, it is (or should be) with the guidance and insights of data analysts. After all, who else really knows what the end product is supposed to be? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euT4WJUOymsJ",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In this lesson we will focus on data pipelines, starting with the general challenges and then highlighting the functional and technical differences between the layers.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **The Seven Vs of Big Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has long been tradition to characterize the unique challenges presented by big data with a laundry list of adjectives, all starting (in English) with the letter *V*. As we have gotten more familiar with the pitfalls of massive datasets, the  list has [expanded to seven](https://towardsdatascience.com/modern-unified-data-architecture-38182304afcc), each motivated by plenty of war stories:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Variety**. Data can come from various disconnected sources, each of which makes its own assumptions about the domain. Even within a given domain, assumptions may change over time. So, what was perfectly valid data a decade ago is not necessarily useful today. \n",
    "- **Velocity**. Modern online systems are really, really good at collecting data. It is like the data tier is trying to \"drink from a firehose\" as it sorts through it all in real-time. Anything that is not stored *immediately* is lost, and not everything can be stored. We have to make hard decisions sometimes. \n",
    "- **Volume**. The shear size of the datasets can be difficult to deal with. We saw that when trying to load the database in the IMDB homework, where the server crashed before the load was complete. In extreme cases, a dataset can be large enough that only part of the data could be loaded at a time. Otherwise, we risk completely filling up the server.  \n",
    "- **Visibility**. There are always going to be new uses for a given dataset. Each time we try to support a new app or new model we may need a new interface, with data formatted a certain way, perhaps with data that has never been shared before. That opens up all sorts of potential bugs and other pitfalls. \n",
    "- **Veracity**. Data can lie. No matter how much we can try to validate every fact, some bugs are going to remain. These kinds of bugs are unknowable and won't surface until uncovered by an application that tries to use the data. \n",
    "- **Vulnerability**. As businesses become more and more dependent on data, the data itself becomes more and more enticing for cyber criminals. Even if they don't hold the data hostage, they may steal secrets that are best left hidden. \n",
    "- **Value**. It is tempting to just dump everything you have into an over-engineered uber data warehouse. However, needs will inevitably change, requiring continual redesign. Maintenance is then a continual process, with each added design feature a potential liability. If a feature does not provide value, then kill it before it becomes a problem instead of a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NekcmKiCsxQm"
   },
   "source": [
    "While any given dataset may be subject to any of these issues, the risks go up exponentially with more data, compounding with each row of data. In other words ... **If it's big data, assume it's pig ugly and expect to spend 80% of your time applying lots of lipstick.**\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Lakes, Warehouses, and Marts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An enterprise-scale pipeline starts in the **data lake** layer, progresses through to a **data warehouse** layer, and then ends with a **data mart** layer. It is possible that a given company may combine or even skip one of these layers, of course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the most common architectural choices are laid out in the table below. \n",
    "\n",
    "|  | Data Lake | Data Warehouse | Data Mart |\n",
    "|---|---|---|---|\n",
    "| **Objective** | Retain Everything | Curate & Integrate Content | Package for Use |\n",
    "| **User Access** | Read-Write | Read-only | Read-Write (views and extracts) |\n",
    "| **Structured Data**| SQL | SQL, OLAP Cubes | SQL, spreadsheets |\n",
    "| **Semi-Structured Data** | JSON, Docs, NoSQL, APIs| SQL (with Extensions), Object-Relational DBs | SQL (with Extensions), NoSQL, Spreadsheets |\n",
    "| **Unstructured Data** | Docs, Text Files, Streams | N/A | NoSQL, Reports |\n",
    "| **Time Scale** | Now/Online | Historical, Online | Historical |\n",
    "| **Storage Requirements** | Petabytes/Terabytes | Terabytes/Gigabytes | Gigabytes/Megabytes |\n",
    "| **Storage Strategy** | Files + Row Stores | Column Stores | Documents / Various |\n",
    "| **Example Tech** |  MySQL, PostgreSQL, AWS DynamoDB, etc. | GCP BigQuery, AWS Dynamo, etc. | Google Sheets, DropBox Files, etc.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Functional Differences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **data lake** is a repository for just about any kind of data. A conscious effort is made to retain the data in its original state, including all the bugs and other errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **data warehouse** is intended as the one true source for all data. It draws from the data lake but then uses business logic to clean it of errors, enrich it with summary facts, and integrate it into a coherent whole. The cleaning and integration are performed as part of the ETL process that loads data into the warehouse. It is important to note that all data enters the data warehouse through the ETL processes. To everything else, access to the data warehouse is strictly read-only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data marts** are generated (extracted) from the data warehouse. Since the warehouse ensures that the data is clean and consistent, there is no need to keep data in a normalized form. In fact, it is usually best to denormalize the data as much as possible (i.e., one table with lots of possibly redundant columns) so as to avoid complex query logic. We can safely do this because the data is never fed back into the data warehouse. The extraction processes that feed the data marts are read-only users just like everybody else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Data Lakehouses** are a new variation on the traditional Lake/Warehouse/Mart architecture where the Lake $→$ Warehouse ETL process is continuous, with all data in the Lake immediately available in the Warehouse. Effectively, this integrates the Lake and Warehouse into a single *Lakehouse*. It also allows analysts to *reverse ETL* analytical results from BI apps back into the Lakehouse, further integrating the entire pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Structure and Format Differences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the data is stored and processed depends in part on the degree of structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structured data** is organized into datasets with a fixed data model (*schema*). Ideally, the data comes from a well-designed relational database so that we can at least assume that it is free of anomalies. If a change is made to the schema then the data is restructured to match. Thus, we always know what the schema is *before writing new data* to a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, structured data is best kept in a relational database, though there are sometimes reasons to consider alternatives. For example, graph databases are excellent at storing geo-spatial data. It is still highly structured, except the schema is not relational. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semi-structured data** has metadata and perhaps a schema but there is no attempt to retain consistency over time. In other words, each datum may have its own schema, which *might not be known until the data is read*. This of course can cause some data integration and retrieval/search issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classic example of a semistructured data format is JSON, which organizes data into hierarchies (trees) of indeterminate depth. It *is* certainly possible to have a consistent structure in JSON but it is not guaranteed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unstructured data** does not have any schema at all. Raw text from a social media stream, for example, can be about anything. Similarly, photos that have not been tagged for content are just collections of pixels and lines. Data retrieval then becomes a matter of luck and intuition rather than a repeatable process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance Requirements and Technology Differences**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At Big Data scale raw performance matters, and how to best deliver that performance depends on where we are in the pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a data lake the emphasis is capturing and storing (writing) data in close to real time. That means any storage strategy needs to accept *serialized* data in the order it is collected. Files are transmitted in serialized form anyway, so they can be stored as is. Relational data (i.e., in a relational database) comes in one transaction at a time, adding rows with each transaction.  MySQL, for example, is tuned to operate this way. It writes (and reads) individual rows of data really quickly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a data warehouse the emphasis is on making queries run as fast as possible. Since most queries only use a few columns at a time, the best way to [structure the data is in columns](https://en.wikipedia.org/wiki/Column-oriented_DBMS). Google BigQuery, for example, uses this strategy to good effect, sometimes offering orders of magnitude speedup over MySQL for the same `SELECT` query. (Python programmers should also note that *pandas* also uses a column-oriented strategy: a DataFrame is equivalent to a dictionary of lists, one list per column.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUNZeFcYCU9A"
   },
   "source": [
    "In a data mart the best strategy depends entirely on the applications. In many cases it might be best to avoid using a database at all, making data available as CSV files, spreadsheets, or other document formats. In any case, the datasets (or databases) themselves are rarely large enough to make performance an issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Data Warehouses are Designed for Analytics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any data warehouse is only as useful as the questions it answers. Whenever possible it should \n",
    "- make calculating aggregate statistics as easy as possible using simple sums, averages, etc.\n",
    "- allow the data to be grouped (labeled) in various ways that make sense to analysts\n",
    "- allow statistics to be disaggregated to identify the base-level source data\n",
    "- use keys and other indexes that are **idempotent** (i.e., time invariant) so that a report from years ago can be rerun today without a major redesign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These requirements lead most naturally to a star schema design where:\n",
    "- There is a central **fact table** with possibly many columns of precomputed aggregable measures (facts) and dimensional labels (foreign keys) that can be used to describe and categorize the facts. The fact table is fine grained, with the facts as atomic as possible. In the stereotypical data warehouse the fact table might have millions of rows. It is also likely to be highly normalized to eliminate potential double-counting errors due to data redundancy. \n",
    "- The fact table is surrounded by **dimension tables** that provide the labels and possibly more descriptive detail. The dimension tables are denormalized to eliminate unnecessary relationships. Usually, each dimension is much smaller than the facts table, with a modest number of rows that rarely change.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call such databases **Dimensional Data Warehouses**, about which we will go into more detail in Lesson 7. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NBA PlayFacts Data Warehouse**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ERD for the NBA PlayFacts warehouse is shown below:\n",
    "- `PlayFact` corresponds to `Event` in the original `PlayLog` data. In a `SELECT` query the facts are what we would aggregate to calculate statistics like a box score. \n",
    "- The `Game`,`Team`, ... dimensions surrounding `PlayFact` represent different ways to aggregate the facts. In a `SELECT` query we would join in these tables as needed, using the primary keys in the `GROUP BY` clause. \n",
    "- The `players_list` attribute is literally a text string with a listing of the players in alphabetical order.  \n",
    "- Any relationships between the dimensions (e.g, games and teams) have been eliminated through selective denormalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NBA PlayFacts Dim DW](./img/L9_NBA_PlayFacts_Star_DW_v2.png\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, why do we want this dimensional design? Because it reduces the vast majority of queries to something like this (where `PlayFact` type $\\rightarrow$ `play_facts` table):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT grouping_columns, aggregate_columns \n",
    "FROM play_facts\n",
    "      JOIN games USING (game_id)\n",
    "      JOIN teams USING (team_id)\n",
    "      JOIN lineups USING (lineup_id)\n",
    "      JOIN players USING (player_id)\n",
    "      JOIN play_segments USING (play_seg_id)\n",
    "      JOIN event_types USING (event_type_id)\n",
    "WHERE row_conditions\n",
    "GROUP BY group_columns\n",
    "HAVING group_conditions\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The joins, which are where most `SELECT` queries go awry, can be the same every time. (Why? Because these joins never add rows to the resultset, they don't introduce double-counting anomalies.) All the analyst has to do is fill in a few details into the template:\n",
    "- the columns to use for the grouping\n",
    "- the columns and functions for the aggregates\n",
    "- the conditions for the rows and groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These details can be configured via a form interface, which usually ends up looking a lot like the ones used for Excel PivotTables (or [Count](https://count.co) if are more notebook-inclined). What could be more convenient (and bulletproof) than that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/L2_excel_pivot_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKpvT0vaTrwH",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "*Source: the MS Excel documentation.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract / Transform / Load (ETL) Processes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data warehouses are meant to be the \"single source of truth,\" integrating all available data. Thus, they are by design based on data collected from somewhere else. Here we build the NBA PlayFacts warehouse from a database with play log data. Then we will show how data can follow a similar ETL process to create data marts for specific purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What follows is technically ELT (Extract / Load / Transform) instead of ETL. This database-centric approach keeps everything in SQL without any need for Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Transactional RDBMS $\\rightarrow$ Dimensional DW**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below shows the original source data (based on the \"play logs\") side by side with the finished data warehouse. Ideally, the source data is already in a DBMS with properly normalized tables (as shown on the left). Such a database is designed to allow new data to be added to it without much risk of corruption. From there it is fairly straightforward to build the data warehouse (on the right), where some data may be safely duplicated for convenience if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Trans 2 Dim ETL](./img/L9_ETL_from_RDBMS_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Pulse Check! A quick review before moving on ...**  \n",
    "Study the \"Transactional DB\" on the left and answer as many of these questions as you can:\n",
    "- Can you spot the subtype/supertype relationship?\n",
    "- Why do we allow multiple events per play segment? Can you give an example? \n",
    "- Can you guess an alternate key for the Teams table?   \n",
    "- As each play segment is recorded, how many tables are (usually) written to? \n",
    "- Which tables could be written to before the start of the game? \n",
    "- How many joins would be necessary to recreate the boxscore from HW2?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the data warehouse on the right, we will need to **transform** the transactional data to fit the new table schema. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SQL `INSERT` and `UPDATE` queries to populate the warehouse tables (right) directly from the existing database (left). \n",
    "- The `play_facts` table is mostly based on the original `events` table, with information from the `play_segments` and `shot_events` tables blended in for convenience. Since the transactional database was highly normalized there is little chance of denormalization causing a data anomaly. \n",
    "- The dimension tables are just slightly denormalized versions of their equivalents in the transactional database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following partial SQL snippet populates the `play_facts` table from tables extracted from the original database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- populating a table in the data warehouse (right)\n",
    "INSERT INTO play_facts (game_id, team_id, ..., p_points, ...)\n",
    "-- using data from the transaction database (left)\n",
    "SELECT game_id, team_id, ... \n",
    "       CASE ... AS p_points,\n",
    "       ...\n",
    "FROM events\n",
    "     JOIN play_segments USING (play_seg_id)\n",
    "     LEFT JOIN `shot_events` USING (event_id);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is actually quite a bit of *transformational* code hidden behind the ellipses (...) but this shows the general pattern: keys copied from existing tables with statistical facts calculated with CASE expressions and functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the queries from getting out of hand, it may be simpler to work in stages, with a first stage that creates the fact rows with the necessary keys, a second stage that updates some of the fact columns, a third stage that handles a few more, etc. This allows each query to be relatively simple, working on a few columns at a time. It also allows some of the earlier stages to inform the later ones, building up complex calculations from simpler ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Warehouse $\\rightarrow$ Data Marts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a data mart from a data warehouse is generally pretty straightforward. In many cases it comes down to a few `SELECT` queries with strategically chosen `GROUP BY` clauses and aggregate calculations, followed by an export to a CSV, spreadsheet, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the NBA data warehouse, we could create a data mart based on just about any of the dimensions: \n",
    "- By game, season, or year\n",
    "- By player, lineup, or team\n",
    "- By event_type, period, or even existence of a phrase in the play description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need is the grouping logic and a process to calculate the group-wise aggregates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `lineup_facts` dataset from Lesson 3 was created to support a research project on the effect of teamwork in basketball. It includes a fully denormalized table (facts w / dimension columns) `play_facts_all`, from which we can calculate various statistics about the usage and efficiencies of basketball lineups over the years. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with the data in Excel or Tableau, however, it is musch easier to use a smaller version that is aggregated by lineup and year. It just barely fits into MS Excel, with just over 200K rows and 35 columns. Besides summarizing (rolling up) the original `play_facts_all` columns, the `season_facts_all` table includes a new column, `plus_minus_36m`, that is needed for the research questions that motivated the study. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F160cmn64672"
   },
   "source": [
    "![Season Facts in Excel](./img/L10_Lineup_Season_Facts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Big Data Tech: Columnar Databases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While any of the logical data models (relational or non-relational) can scale up for Big Data applications, they are only partial solutions. To complete the job, we also need *technology* (hardware and software) that can scale up as well. For that we suggest using a columnar database like Google BigQuery or AWS RedShift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Row Stores vs Column Stores**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While all relational technology takes care not to use excessive storage space, there is a noticable difference between transactional databases and analytical databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transactional databases** work best using write-optimized **row store** technology. Data is continually being written to the database, one row at a time. In the example below each arrow represents one read or write operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Row Store](./img/L11_RowStore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of how transaction control works, any delays in writing new rows also affect any other query that might be executing at the time. Each successive delay then takes up more and more computing capacity, causing further delays. Thus, in order to minimize the risk of system failure (dropped queries and rollbacks), it is best to prioritize writing data over reading it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical databases** tend to use read-optimized **column store** (or *columnar*) technology. Here the emphasis is on reading data organized into columns. Writing individual rows, however, can be very expensive because adding one row writes to multiple columns, which may be very far apart in physical storage. Thus in typical usage data rows will be added in bulk rather than one at a time. That requires just one write operation per column (shown in red below). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Column Store](./img/L11_ColumnStore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Heads Up:** There is a common misperception, mostly among old-school data engineers, that column stores are non-relational. **The relational database model is about logic, not physical implementation.** Columnar databases implement all of the defining features and functions of the relational model and are thus *by definition* relational databases. The decision to store data in rows or columns is an implementation detail that has absolutely nothing to do with the relational database model.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Performance Optimizations**\n",
    "While columnar databases do not excel at writing new data, they are exceptionally good at `SELECT` queries and raw data storage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most select queries only use a few columns of any given table.** For a dimensional data warehouse, where the fact tables tend to have many columns, that means that only a small fraction of the table needs to be processed (i.e., in memory) at a time. Let's say that we have a fact table with 50 measure (non-key) columns. If we are only using 2 columns, then the query processor has a lot less work to do in order to carry out a query. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A big advantage of columnar storage is that it makes data compression really simple.** Data compression relies on exploiting repeated patterns in data. If the same pattern is repeated many times, then we can keep one copy of the pattern and then record each place where it applies. That alone can save space but we can go much further if the same pattern is repeated many times in a row. Consider, for example, the `Rating` column of our movies data. We see that 'PG-13' is repeated 27 times in a row. That means that the database only needs to store:\n",
    "- the string 'PG-13'\n",
    "- a \"run length\" of 26 more repeats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a compression ratio of about 26:1. Similar logic works for numeric data as well. Typically, numeric data tends to appear in somewhat narrow ranges, with each value in a column similar to the ones before and after it. This allows us to store the numbers as differences (above or below) some base value. Since the differences are small they can take up less storage space. For example, consider the `ShowTime` column in our example. In SQL, dates and times actually get stored as numbers (in order to simplify date/time arithmetic). However, since the show times are fairly regular, we can \n",
    "- calculate the difference between each showtime and the one above it to produce a column of mostly zeros, and then \n",
    "- compress the column even further using run lengths on the zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be quite impressive, though not quite as good as those for columns of text data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Columnar databases don't work as well with inherently row-oriented operations like table joins.** Thus, some data warehouse designs eschew _dimension tables_ (and foreign keys) in favor of _dimension columns_, with a fully denormalized, one-table design where _everything_ is kept in the fact table. While that certainly works, it can potentially cause other problems if the source data has any anomalies. So, in recent years, columnar database solutions have implemented **optimized views that do the joins in advance**. This is done behind the scenes, in a way that is invisible to the user. For a dimensional data warehouse where the joins are the same every time anyway, such a strategy provides the speed advantage of column-wise storage and retrieval without giving up the integrity guarantees of table normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Columnar Database Examples**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Google BigQuery (used for many of the NBA examples in this course, see below)\n",
    "- AWS Redshift\n",
    "- Snowflake\n",
    "- Azure SQL Data Warehouse\n",
    "- Oracle Autonomous Datawarehouse \n",
    "- MariaDB ColumnStore\n",
    "- PostgreSQL cstore_fdw\n",
    "- Teradata\n",
    "- Vertica\n",
    "- Yellowbrick Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SnxkvM4hLobH"
   },
   "source": [
    " It is also worth noting that pandas DataFrames and R Data Frames take the same column-centric approach (for the same reasons and with the same benefits/challenges) but without SQL. So, if you are comfortable using data frames for your data science projects you should be able to pick up any of these Columnar databases with only a small learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Beyond SQL: Data Warehouse $\\rightarrow$ Tableau**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will conclude this lesson by building a visualization model using data drawn from our data warehouse. Rather than export the data into a data mart as a CSV file, we will use a query to pull data directly from a Google BigQuery data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/L9_NBA_PLayFacts_Pipeline_v2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tool of choice is [Tableau](https://tableau.com). From the website:\n",
    "> Tableau helps people see and understand data. Our visual analytics platform is transforming the way people use data to solve problems. See why organizations of all sizes trust Tableau to help them be more data-driven."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using [Tableau Desktop](https://www.tableau.com/products/desktop), the fully-featured version intended for professional use. It is available with free [academic licensing](https://www.tableau.com/academic/students) for  up to one year of classroom use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Heads Up:** There is also a free Tableau Public version, which works much the same as Tableau Desktop *except* that it doesn't include the ability to extract data from online sources using SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### **Connecting to the Data Warehouse**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tableau organizes its analytics models into projects that bundle sheets (visual models), dashboards, and stories. In this quick example we will only need a single sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started we will connect the sheet to our NBA Lineup Facts database in BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Connect to BigQuery](./img/L9_Tableau_Connect_to_Data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tableau will open a browser window to log you into your Google account and authorize you to access the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **As of this writing, BigQuery is not accessible in China, but it might be accessible through a cloud-hosted app like Tableau Public or an AWS service.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Running a Custom SQL Query**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once connected, we can select *New Custom SQL* from the data source panel to create a new `SELECT` query.  Instead of drawing on the full data warehouse, the query below returns aggregated data \"rolled up\" by season. The aggregated data is in effect a **data mart** stored in the same database instance as the data warehouse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SELECT Query](./img/L9_Tableau_Custom_Query.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Heads Up:** Tableau only allows one `SELECT` statement per query. It will even throw an error if you supply a semicolon at the end of the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the query (by clicking OK), we are presented with the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Query Results](./img/L9_Tableau_Query_Results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Heads Up:** Tableau will complain if the extracted dataset has a large number of rows. Tableau is designed for visualization, not data management. If there is too much data to display on a chart then Tableau is nudging us to be more selective with the `SELECT` query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Building the Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Tableau sheet is a lot like using Excel's PivotTable and charting features:\n",
    "1. Identify fields as either (categorical) dimensions or (continuous) measures. \n",
    "2. Drag fields to the Columns and Rows areas to define the vertical and horizontal axes of the plot. \n",
    "  - Dimensions are used to label or group the data\n",
    "  - Measures are used for aggregation (sums, averages, etc.) and plotting\n",
    "3. If we only want a subset of the data then we can set one or more filters.\n",
    "4. Use the \"Show Me\" popdown panel to select the desired visualization model.\n",
    "5. Configure labels, colors, etc. as needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have selected a scatter plot with the `play_length_mins` for the horizontal axis and `plus_minus_36m` for the vertical axis. We have also filtered to only the lineups with 200+ minutes played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Build Model](./img/L9_Tableau_Scatter_Plot_Filter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The (mostly) Finished Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except for a few tweaks like giving the sheet a name, adjusting the tick marks, etc. the model is done within about 5 minutes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Finished Model](./img/L9_Tableau_Scatter_Final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Now for the Money Shot ...**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finished plot is given below, which uses a few advanced features to tell a story about the performance of starting and closing lineups:\n",
    "- Each dot represents a lineup (i.e., distinct combinations of five players on the court at the same time) with at least 200 minutes played. There are almost 900 such lineups in the dataset. Most are so-called \"Starting\" lineups but some of the best are not. \n",
    "- The horizonal axis is total minutes played. The lineups to the far right, at the tip of the arrowhead, played the most minutes together in a season.\n",
    "- The vertical axis is efficiency, measured as \"plus/minus per 36 minutes\" ('$\\pm$36m') over a full season. By that measure the most efficient in history is a 2011 Dallas Mavericks lineup that was only used in the playoffs. That year they won the NBA championships over what many considered a better Miami Heat team (led by Lebron James). It was no upset though. Dallas had the better lineup. \n",
    "- The big gold dots represent the famous Golden State death lineups. These were so good that many said they should be outlawed.\n",
    "- The smaller red dots are lineups with LeBron James.  \n",
    "- The blue line at the top (called the tradeoff curve) shows the best performance found at each minute threshold (i.e., maximum performance where minutes <= threshold). Each lineup on that curve could be said to be the best in history! Or at least since 2004."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnkGOtS8UwQ-"
   },
   "source": [
    "![nba lineup tradoff](./img/L3_nba_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVL7n_Rys_95"
   },
   "source": [
    "---\n",
    "## **Congratulations! You've made it to the end of Lesson 8.**\n",
    "\n",
    "Next time we will focus more on data architecture, with a moderately deep dive into star schema models.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNE3pktSoH0h+1F3SBrB0C8",
   "mount_file_id": "11MBPducEj0HgT0u-nrCE60AvHJQnPR3y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
