{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/Dolan.png\" width=\"180px\" align=\"right\">\n",
    "\n",
    "# **DATA 6510**\n",
    "# **Lesson 7: Dimensional Data Warehouses** \n",
    "_Facts, Dimensions, and Cubes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rHzwUorF5ab"
   },
   "source": [
    "## **Learning Objectives**\n",
    "### **Theory / Be able to explain ...**\n",
    "- How database design for analytical applications is different from that for transaction systems\n",
    "- The various forms of the star schema and when each is most applicable\n",
    "- The concept of data granularity in dimensional data warehouses\n",
    "- Data cubes as a dimensional data model \n",
    "\n",
    "### **Skills / Know how to ...**\n",
    "- Identify dimensions that provide context to facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **BIG PICTURE: The Holy Grail of Data Warehousing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Warehousing as the \"one true source of truth\" is an idea that goes all the way back to the 1980s. Analysts had by that point built up quite a repertoire of models for just about any kind of analysis. They could create classification and regression trees (decision trees, random forests, etc.). They could do linear, nonlinear, kernel, and logistic regression with large datasets. They could solve optimization problems with thousands of variables and tens of thousands of constraints. Even the neural network models at the core of the latest and greatest deep learning techniques were pretty mature by 1994.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was missing was data! Well, sort of. Mainframe systems had been collecting transaction data for decades. Every day, banks and credit card companies were processing millions of transactions, transportation systems were tracking hundreds of thousands of shipments, etc. All we had to do was archive it all so we could analyze it later. Or so it seemed to most of us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analytical data is not the same as transactional data, of course, in ways that were clear at the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   | Transactional Data | Analytical Data |\n",
    "|---| --- |--- |\n",
    "| **Scope** | Functional / Operational | Strategic / Executive |\n",
    "| **Provenance** | Online / Production | Historical + 3-rd Party |\n",
    "| **Operating Environment** | Enterprise / Big Iron | Workspace / Workstation |\n",
    "| **Data Quality** | \"good enough to run the company\" | \"small errors compound over time\" |\n",
    "| **Performance Objective** | maximize transactions per hour | minimize time to results |\n",
    "| **Access to Datasets** | read-only queries and reports | offline, with ability to make corrections |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where would anybody get the analytical data with the right scope, volume, quality, etc. quickly and easily enough to useful? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even back then the ultimate solutions were known, though not remotely close to being available. Consider, for example, this figure from Ralph Kimball's seminal book *The Data Warehouse Lifecycle Toolkit*, published in 1998 and based on original work **from the mid-1980s**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Kimball's Data Warehouse](./img/L10_Kimball_Data_Warehouse_Elements.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the elements of a modern data pipeline are there. It even articulated the steps of the ETL process in detail. It was all there ... to be realized *someday*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Someday is now. With commodity data storage, ample computing power, ready-made software for just about any kind of modeling, and the analytical results to attract attention from management, data infrastructure is finally seen as what it should have been all along: a critical resource upon which the company relies to make it stand out against the competition. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what the vendors tell us, anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euT4WJUOymsJ"
   },
   "source": [
    "In this lesson we will explore the **Dimensional Data Warehouse** model first proposed by Kimball et al. all those years ago. We will also consider how its mass adoption has influenced the SQL standard in recent years, with the addition of **window functions** and **collection** data types (Lesson 8) that relax fundamental assumptions of the relational data model in favor of analytical use cases. \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **The Star Schema Pattern**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some level, nearly all data warehouses look roughly the same. There is a huge table in the center with lots of columns and foreign key relationships to  smaller tables around the periphery. This general pattern is called a **star schema**. A relational database that implements the star schema pattern (or one of its variants) is called a dimensional data warehouse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Heads Up**: The star schema is a **design pattern, a standard solution to a standard problem.** By standardizing what they call these solutions, designers can communicate among themselves with just a few words instead of re-explaining the details every time. It is both more efficient and potentially less error-prone, as any standard should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Star Schema pattern features two kinds of tables:\n",
    "- **A fact table with precomputed quantitative measures.** The measures themselves are somewhat volatile, with new measures continually added and others redefined to suit the ever changing needs of the analysts. If there is a way to precompute a statistic or other measure so analysts don't have to, then do it. If a given measure is no longer needed or misleading, then we redefine or remove it. \n",
    "- **Dimension tables that provide context for the facts.** Dimensions are somewhat timeless and immutable. Even when the facts themselves may change over time, the dimensions remain relatively static. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the fact table has a foreign key reference to each of the dimension tables. Dimension tables, meanwhile, stand on their own, without any foreign keys. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The star schema addresses the disconnect between the way data is recorded versus how it is used by analysts. It reduces all data down to measures (facts) and context (dimensions). Since *all information* takes that form eventually, star schemas strike a nice balance between structure and general applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, here is the NBA PlayFacts warehouse, this time noting some of the key features. We will use it as an example, starting with the dimensions before moving on to the facts. We will also explore variations on the general star schema pattern that fit certain use cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCSbx4saW1Hu"
   },
   "source": [
    "![NBA PlayFacts Dim DW](./img/L10_Star_Schema_Notes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Dimension Tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensions are the lens through which we interpret each fact. They are what give it context and meaning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In theory the **dimensions are strong entities that exist independently of the facts**. Each fact, meanwhile, represents a collection (more like a selection) of details found in the dimensions. We'll go deeper into this when get discuss fact tables and granularity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though there is some disagreement about this, **the usual recommendation is that dimension tables be fully _denormalized_**. Since they are often fairly small (relative to the fact table) and don't change much, there is little chance of creating anomalies over time. So, while it may be tempting to, for example, normalize out zip codes and cities from a location dimension, there is no real need, especially when it would require an unnecessary table join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what kinds of details are we talking about? A good starting point is the framework used by journalists and storytellers the world over:\n",
    "- **Who was involved?** People, roles, etc. \n",
    "- **What happened** Event types, outcomes, etc. \n",
    "- **When did it happen?** Timing or place in a sequence\n",
    "- **Where did it happen?** Location, which may be conceptual rather than physical\n",
    "- **Why did it happen?** Intent, cause, etc. \n",
    "- **How did it happen?** Steps, sequential logic, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the who/what/when/where/why/how questions can suggest multiple dimensions. As we can see in the NBA example, the question of \"who is involved in a given play?\" is answered with *three* dimensions: \n",
    "- the individual player who gets credited with each event\n",
    "- the lineup of players on the court at the time\n",
    "- the team whose play is being reflected by the facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is done this way to support different, independently-calculated measures:\n",
    "- the counting stats (points, rebounds, assists, etc.) for an individual player, lineup, or team\n",
    "- the total playing time for each player or lineup  on the court, even when they are not generating counting stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How Many Dimensions? Granularity and Focus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An often overlooked but potentially tricky aspect of dimensional design is whether dimensions are allowed to overlap. In other words, can the same dimension be represented two different ways? Can we combine dimensions to create a third uber-dimension? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like a lot of things, it depends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to combine dimensions:** Some dimensions may permit several levels of *granularity*. For example, a given office is in a region, which is in a division, etc. To borrow terminology from normalization, we say that there is a chain of functional dependencies: office $\\rightarrow$ region $\\rightarrow$ division. The recommendation is to keep the levels together in a single dimension table rather than separate them out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This would apply, for example, to the way NBA teams are grouped into *divisions* and *conferences* (Eastern or Western) that affect scheduling decisions. In other words, team $\\rightarrow$ division $\\rightarrow$ conference. Since each group nests cleanly inside the next, there is no need to separate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When to separate into multiple dimensions:** Within a company the *location* of a fact can mean geolocation (addresses) or a spot in organizational hierarchy (functional area, group, etc.). While they are both location dimensions, they are not logically connected via a functional dependency. They should reside in separate dimension tables, not one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have already seen this case with the NBA players, lineups, and teams. There are many-to-many relationships among them, making it impossible to create a consistent functional dependency chain; they don't nest together cleanly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Slowly Changing Dimensions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of specialized types of dimensions (conformed, role-playing, junk, etc.) that serve different purposes. Most of the time, however, we are working with so-called **_slow moving_** dimensions where the rows and columns don't change much over time. Often such dimensions can be maintained by hand or perhaps via a periodic updating process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, while NBA players do occasionally move from team to team or sign \"10 day contracts\" to replace injured players, for the most part the team rosters do not change much during the season. We can maintain them as needed, starting with a preliminary roster at the start of the seaosn and then adding to it as new players join the team. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHB_1psdXByM"
   },
   "source": [
    "Similarly, NBA games are scheduled released well in advance, allowing us to update the games dimension all at once with perhaps a rare game reschedule if needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## **Fact Tables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact tables exist at the intersection of the dimension tables.** Each fact is labeled with foreign keys, usually one key per dimension. The rest of the columns are measures that can be used in aggregate calculations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What makes a good measure? Anything for which we can calculate descriptive statistics (counts, averages, etc.): \n",
    "- For text data, we generally are limited to the text itself and counts of some sort. We may, for example, count the number of times the word \"no\" appears, how many sentences there are, etc. \n",
    "- For numerical data we can use all of the usual statistics like mean, maximum, minimum, etc., or perhaps *bin* into nonoverlapping categories (or *segments*). \n",
    "- For temporal data (dates and times), we may calculate elapsed times, inter-event times, cumulative times, etc. that can be treated like numerical data.\n",
    "- For binary data (pictures, etc.), the options are very limited, though one may be able to apply a machine learning technique to generate numerical digests that can be aggregated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the fact table can only be as **granular** as the **dimensions** allow. In other words, if a given dimension only has 3 possible labels (rows), then that dimension can only divide up the facts three ways. We can, however, increase the granularity by adding **new** dimensions. Each additional dimension potentially increases the granularity but never decreases it. Alternatively, we can increase the granularity by adding strategically selected rows to an **existing** dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to visualize this is with a (hyper-) cube, with each dimension on a side. Each fact is *binned* inside one of the smaller cubes at the intersection of the dimensions. For the NBA PlayFacts cube below, each fact is binned based on the game, team, and player. Thus, if with only three dimensions, we would only be able to generate box score stats for full games. In order to get statistics within a game (e.g., for the last two minutes of each period) we would need to include a *play segment* dimension. (Don't ask about how we'd show a 4-dimensional cube. Just know that we can.) Thus, by adding a dimension we have also increased the granularity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Data Cube](./img/L10_DataCube_wide.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Heads Up:** It is sometimes difficult to distinguish dimensions from measures when source data is numerical. For example, is the time on the clock (i.e., seconds remaining in the period) a measure or a dimension? It is a measure, in the sense that it captures the passage of time, but it is also a dimension, in that it records when a given event happened. The key when considering whether any given quantity belongs on the fact table is to ask whether you would i) aggregate it (sum, average, etc.) or ii) cite it. In a basketball game it is the latter (cited), so we separate it out into the play segment dimension. The clock *interval* between events (elapsed time), however, is something that we can sum up by quarter, player, etc. Thus, it belongs on the fact table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Rollups and Drilldowns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the big advantages of a dimensional data warehouse design is that it makes it very simple to aggregate and disaggregate data at various levels of granularity. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Heads Up**: The terms \"rollup\" and \"drilldown\" were coined by vendors of OLAP (OnLine Analytical Processing) systems that do real-time ETL from transaction data but the terms can be readily applied to any dimensional data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **rollup** is the standard `GROUP BY` aggregation operation. The idea is that a whole stripe of data in the cube (i.e., a dimension) is \"rolled up\" like a carpet and then replaced with summary data. We can do this for several dimensions at a time to get summarized data of various purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **drilldown** is the opposite of a rollup. Starting with aggregate data, a drilldown disaggregates it to a finer level of granularity. Behind the scenes, it is the same as a rollup, just with a lower level of aggregation. Ultimately, the lowest level of a drilldown is the fact table itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NekcmKiCsxQm"
   },
   "source": [
    "---\n",
    "## **Variations**\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dimension *Tables* vs Dimension *Columns***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the advantages of keeping dimensional data in separate tables is that it can significantly reduce storage costs by eliminating redundant data. However, with the advent of cheap cloud-based data storage, cost becomes less important than performance. Thus, we may choose to denormalize everything into a single table (or materialized view) that doesn't require any expensive joins. It's simple enough. If we already have the data in a normalized form, then we would just need to join in every table and select every column to generate a new \"one table fits all\" data warehouse. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A materialized view is like a self-updating table; see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "CREATE MATERIALIZED VIEW denormalized_fact_table AS\n",
    "  ( SELECT *\n",
    "    FROM fact_table \n",
    "      JOIN dimension_a ...\n",
    "      JOIN dimension_b ...\n",
    "      JOIN ...\n",
    "  );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensions would still be there, just as columns instead of tables. The joins would be completed in advance, simplifying `SELECT` queries even further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that in practice we would not use * in a view object; it's much more efficient to list specific columns. Also, materialized views need to be refreshed just before use to avoid stale data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While denormalized views sound great in theory, there are two good reasons for creating separate dimension tables: \n",
    "- **If care is taken to *use just foreign keys in the `GROUP BY` clause*, then it can *sometimes* be actually be faster to query multiple tables than a single table.** This is because the joins will happen **after** the grouping has reduced the data to a manageable number of fact table rows. The incremental performance cost of the join is then practically nil, especially if there are a small number of groups in the result set. \n",
    "- **Dimension tables provide opportunities to add in static descriptive data.** For example, we could add in the seating capacity or age of a given basketball arena if we treat it as a dimension table instead of just a column. If an arena were just a few columns then we'd have to update the view each time we added a column. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLgrt7SbXMvV"
   },
   "source": [
    "Whether either of these advantages are relevant depends on the situation. As a general rule, unless you have a good reason not to, it is best to create dimension tables instead of dimension columns. You can always create a view if needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Snowflakes and Galaxies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule that dimensions be denormalized is more of a convenience than a law. The purpose is to make writing queries as simple and bug-free as possible. However, if we are ultimately going to denormalize the data using the same joins every time, then what difference does it make how many joins there are? Such is the reasoning behind the so-called **Snowflake design pattern**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, for example, is the snowflake version of the PlayFacts database. The dotted lines show relationships normalized out from the star schema dimensions. There is no effect on the fact table, just more detail in the dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/L10_NBA_PlayFacts_Snowflake_DW.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages of normalizing the dimensions include:\n",
    "- less data redundancy, smaller storage requirements, etc. that we usually associate with normalization\n",
    "- the ability to add new details (like team franchises or arenas) without disrupting existing dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to point out that the snowflake and star schema patterns are both perfectly valid. Which to use is totally situational. Let the data, its usage, and performance considerations guide your decision. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the snowflake pattern, the **galaxy pattern** also extends the star schema, this time by allowing multiple fact tables. The baseball database from lesson 2 is a galaxy with ...\n",
    "- Fact tables for fielding, batting, pitching, etc.  \n",
    "- Dimension tables for players (Master), teams, all star game appearances, etc.  \n",
    "\n",
    "It also follows the snowflake pattern, with enhancements like All Star Game appearances, Hall of Fame voting, etc. attached to the players and teams. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bz9O8d7XSev",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "![Lahman 2016 ERD](./img/L2_baseball_stats_schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVL7n_Rys_95"
   },
   "source": [
    "---\n",
    "## **Congratulations! You've made it to the end of Lesson 7.**\n",
    "\n",
    "Next week we will consider alternative data models that can improve flexibility and performance. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO4cyvgW5Mo9If7yewqblRW",
   "mount_file_id": "11MBPducEj0HgT0u-nrCE60AvHJQnPR3y",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
